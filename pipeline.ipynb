{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vedasunkara/EBUReproducibilityChallenge/blob/master/pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tMvSguLvTY2A",
        "colab_type": "text"
      },
      "source": [
        "Veda attempts to make a GAN that works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmGPlFZ_wXug",
        "colab_type": "code",
        "outputId": "4ae90f53-3441-4d3d-97a8-7a7619888380",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BGUSVVI2wZIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the Pandas libraries with alias 'pd' \n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "\n",
        "# parse data from csv\n",
        "def process_data():\n",
        "  # x-values\n",
        "  headers = [\"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
        "             \"marital-status\", \"occupation\", \"relationship\", \"race\",\n",
        "             \"sex\", \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
        "             \"native-country\", \"wage\"]\n",
        "\n",
        "  df = pd.read_csv('/content/drive/Shared drives/RL Research (Veda & Naveen)/adult.data', names=headers, delim_whitespace=True, index_col=False)\n",
        "  \n",
        "  def normalize(df):\n",
        "    return (df - df.mean()) / (df.max() - df.min())\n",
        "\n",
        "  df[\"age\"] = df[\"age\"].astype('category').cat.codes\n",
        "  normalized_age = normalize(df[\"age\"])\n",
        "  df = df.drop(\"age\", axis=1)\n",
        "  df = df.join(normalized_age)\n",
        "\n",
        "  df[\"workclass\"] = df[\"workclass\"].astype('category').cat.codes\n",
        "  one_hot_workclass = pd.get_dummies(df[\"workclass\"])\n",
        "  df = df.drop(\"workclass\", axis=1)\n",
        "  df = df.join(one_hot_workclass)\n",
        "\n",
        "  df[\"fnlwgt\"] = df[\"fnlwgt\"].astype('category').cat.codes\n",
        "  normalized_fnlwgt = normalize(df[\"fnlwgt\"])\n",
        "  df = df.drop(\"fnlwgt\", axis=1)\n",
        "  df = df.join(normalized_fnlwgt)\n",
        "\n",
        "  df[\"education\"] = df[\"education\"].astype('category').cat.codes\n",
        "  one_hot_education = pd.get_dummies(df[\"education\"])\n",
        "  df = df.drop(\"education\", axis=1)\n",
        "  df = df.join(one_hot_education, rsuffix='_right')\n",
        "\n",
        "  df[\"education-num\"] = df[\"education-num\"].astype('category').cat.codes\n",
        "  normalized_education = normalize(df[\"education-num\"])\n",
        "  df = df.drop(\"education-num\", axis=1)\n",
        "  df = df.join(normalized_education)\n",
        "\n",
        "  df[\"marital-status\"] = df[\"marital-status\"].astype('category').cat.codes\n",
        "  one_hot_marraige = pd.get_dummies(df[\"marital-status\"])\n",
        "  df = df.drop(\"marital-status\", axis=1)\n",
        "  df = df.join(one_hot_marraige, rsuffix='_right')\n",
        "\n",
        "  df[\"occupation\"] = df[\"occupation\"].astype('category').cat.codes\n",
        "  one_hot_job = pd.get_dummies(df[\"occupation\"])\n",
        "  df = df.drop(\"occupation\", axis=1)\n",
        "  df = df.join(one_hot_job, rsuffix='_right')\n",
        "\n",
        "  df[\"relationship\"] = df[\"relationship\"].astype('category').cat.codes\n",
        "  one_hot_relationship = pd.get_dummies(df[\"relationship\"])\n",
        "  df = df.drop(\"relationship\", axis=1)\n",
        "  df = df.join(one_hot_relationship, rsuffix='_right')\n",
        "\n",
        "  df[\"race\"] = df[\"race\"].astype('category').cat.codes\n",
        "  one_hot_race = pd.get_dummies(df[\"race\"])\n",
        "  df = df.drop(\"race\", axis=1)\n",
        "  df = df.join(one_hot_race, rsuffix='_right')\n",
        "\n",
        "  df[\"sex\"] = df[\"sex\"].astype('category').cat.codes\n",
        "  one_hot_sex = pd.get_dummies(df[\"sex\"])\n",
        "  one_hot_sex = one_hot_sex.rename(columns={0: \"naveen\", 1: \"veda\"})\n",
        "  df = df.drop(\"sex\", axis=1)\n",
        "  df = df.join(one_hot_sex, rsuffix='_right')\n",
        "\n",
        "  df[\"capital-gain\"] = df[\"capital-gain\"].astype('category').cat.codes\n",
        "  normalized_gain = normalize(df[\"capital-gain\"])\n",
        "  df = df.drop(\"capital-gain\", axis=1)\n",
        "  df = df.join(normalized_gain)\n",
        "\n",
        "  df[\"capital-loss\"] = df[\"capital-loss\"].astype('category').cat.codes\n",
        "  normalized_loss = normalize(df[\"capital-loss\"])\n",
        "  df = df.drop(\"capital-loss\", axis=1)\n",
        "  df = df.join(normalized_loss)\n",
        "\n",
        "  df[\"hours-per-week\"] = df[\"hours-per-week\"].astype('category').cat.codes\n",
        "  normalized_hours = normalize(df[\"hours-per-week\"])\n",
        "  df = df.drop(\"hours-per-week\", axis=1)\n",
        "  df = df.join(normalized_hours)\n",
        "\n",
        "  df[\"native-country\"] = df[\"native-country\"].astype('category').cat.codes\n",
        "  one_hot_country = pd.get_dummies(df[\"native-country\"])\n",
        "  df = df.drop(\"native-country\", axis=1)\n",
        "  df = df.join(one_hot_country, rsuffix='_right')\n",
        "\n",
        "  x_vals = df.drop(\"wage\", axis = 1)\n",
        "\n",
        "  # original values\n",
        "  y_vals = df[\"wage\"].astype('category').cat.codes\n",
        "  y_vals = y_vals.fillna({\"wage\": -1})\n",
        "\n",
        "  return x_vals, y_vals, x_vals['veda']\n",
        "\n",
        "# process data to just get women who make > 50k\n",
        "def get_data():\n",
        "  x, y, _ = process_data()\n",
        "  x = x.to_numpy()\n",
        "  y = y.to_numpy()\n",
        "  \n",
        "  training_inputs = []\n",
        "  training_labels = []\n",
        "  \n",
        "  # only get data for women who make > 50K\n",
        "  for i in range(len(x)):\n",
        "    if(x[i, 67] == 0 and y[i] == 1):\n",
        "      training_inputs.append(x[i])\n",
        "      training_labels.append(y[i])\n",
        "     \n",
        "  return np.array(training_inputs), np.array(training_labels)\n",
        "\n",
        "# process_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWmSZbv3TWIm",
        "colab_type": "code",
        "outputId": "b966681a-3306-46ce-c7f4-687599ce02ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from __future__ import print_function, division\n",
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "from keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GAN():\n",
        "  def __init__(self):\n",
        "    self.latent_dim = 100\n",
        "    self.output_dim = 108\n",
        "    self.num_classes = 2\n",
        "    \n",
        "    self.optimizer = Adam(0.001)\n",
        "    \n",
        "    # Build and compile the discriminator\n",
        "    self.discriminator = self.build_discriminator()\n",
        "    self.discriminator.compile(loss=['binary_crossentropy'], optimizer=self.optimizer, metrics=['accuracy'])\n",
        "    \n",
        "    # Build the generator\n",
        "    self.generator = self.build_generator()\n",
        "    \n",
        "    # generator takes in noise and generates a 14-pt feature vector for a person\n",
        "    noise = Input(shape=(self.latent_dim,))\n",
        "    label = Input(shape=(1,))\n",
        "    output = self.generator([noise, label])\n",
        "    \n",
        "    # determine the validity of the output using the discriminator\n",
        "    valid = self.discriminator([output, label])\n",
        "    \n",
        "    # combine generator and discriminator to train together\n",
        "    self.combined = Model([noise, label], valid)\n",
        "    self.combined.compile(loss=['binary_crossentropy'], optimizer=self.optimizer)\n",
        "\n",
        "  def build_generator(self):\n",
        "    model = Sequential()\n",
        "    \n",
        "    model.add(Dense(256, input_dim=self.latent_dim))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(1024))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(BatchNormalization(momentum=0.8))\n",
        "    model.add(Dense(np.prod(self.output_dim), activation='tanh'))\n",
        "    model.add(Reshape((self.output_dim,)))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    noise = Input(shape=(self.latent_dim,))\n",
        "    label = Input(shape=(1,), dtype='int32')\n",
        "    label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "\n",
        "    model_input = multiply([noise, label_embedding])\n",
        "    output = model(model_input)\n",
        "    \n",
        "    return Model([noise, label], output)\n",
        "  \n",
        "  def build_discriminator(self):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Dense(512, input_dim=np.prod(self.output_dim)))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(512))\n",
        "    model.add(LeakyReLU(alpha=0.2))\n",
        "    model.add(Dropout(0.4))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    \n",
        "    model.summary()\n",
        "    \n",
        "    inp = Input(shape=(self.output_dim,))\n",
        "    \n",
        "    # label is 0 or 1, =< or > 50K\n",
        "    label = Input(shape=(1,), dtype='int32')\n",
        "    \n",
        "    label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.output_dim))(label))\n",
        "\n",
        "    model_input = multiply([inp, label_embedding])\n",
        "    validity = model(model_input)\n",
        "    \n",
        "    return Model([inp, label], validity)\n",
        "  \n",
        "  def generate_data(self, num_datapoints=1000):\n",
        "    noise = np.random.normal(0, 1, (num_datapoints, 100))\n",
        "    labels = np.ones(num_datapoints)\n",
        "    \n",
        "    return self.generator.predict([noise, labels]), labels\n",
        "    \n",
        "  def train(self, epochs, batch_size=128):\n",
        "    x_train, y_train = get_data()\n",
        "    \n",
        "    # Adversarial ground truths\n",
        "    valid = np.ones((batch_size, 1))\n",
        "    fake = np.zeros((batch_size, 1))\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "      # train the discriminator\n",
        "      idx = np.random.randint(0, np.shape(x_train)[0], batch_size)\n",
        "      inputs, labels = np.take(x_train, idx, axis=0), np.take(y_train, idx)\n",
        "      \n",
        "      # Sample noise as generator input\n",
        "      noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "      # Generate a half batch of new images\n",
        "      gen_data = self.generator.predict([noise, labels])\n",
        "\n",
        "      # Train the discriminator\n",
        "      d_loss_real = self.discriminator.train_on_batch([inputs, labels], valid)\n",
        "      d_loss_fake = self.discriminator.train_on_batch([gen_data, labels], fake)\n",
        "      d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "      \n",
        "      #train generator\n",
        "      # Condition on labels\n",
        "      sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
        "\n",
        "      # Train the generator\n",
        "      g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
        "\n",
        "      # Plot the progress\n",
        "      print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "\n",
        "gan = GAN()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 512)               55808     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 512)               262656    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 1)                 513       \n",
            "=================================================================\n",
            "Total params: 581,633\n",
            "Trainable params: 581,633\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 256)               25856     \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)    (None, 1024)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 1024)              4096      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 108)               110700    \n",
            "_________________________________________________________________\n",
            "reshape_1 (Reshape)          (None, 108)               0         \n",
            "=================================================================\n",
            "Total params: 800,620\n",
            "Trainable params: 797,036\n",
            "Non-trainable params: 3,584\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLwOi19Csnv8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dropout\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "\n",
        "class PredictionModel():\n",
        "  def __init__(self):\n",
        "    self.b_size = 32\n",
        "    self.input_dim = 108\n",
        "    self.model = self.build_model()\n",
        "  \n",
        "  def build_model(self):\n",
        "    model = Sequential()\n",
        "    model.add(Dense(32, input_dim=self.input_dim, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(32, activation=\"relu\"))\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Dense(2, activation=\"softmax\"))\n",
        "    \n",
        "    model.summary()\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='categorical_crossentropy', \n",
        "        metrics=['accuracy'],\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "  def train(self, inputs, labels, inputs_test, labels_test):\n",
        "    self.model.fit(inputs, labels, batch_size=64, epochs=1, validation_data=(inputs_test, labels_test))\n",
        "  \n",
        "  def test(self, inputs):\n",
        "    return self.model.predict_proba(inputs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9M1BMMv-pPaG",
        "colab_type": "text"
      },
      "source": [
        "Loop, based on KL-divergence, and generate fake data as necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oO5dksFPpO1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def bin_probabilities(predictions, gender):\n",
        "  sex_m_x = np.zeros(10)\n",
        "  sex_f_x = np.zeros(10)\n",
        "  \n",
        "  for i in range(predictions.shape[0]):\n",
        "    pred = predictions[i][1] #grab probability that individual earning more than 50K\n",
        "    loc = 0\n",
        "    if (pred <= .1):\n",
        "      loc = 0\n",
        "    elif (pred <= .2):\n",
        "      loc = 1\n",
        "    elif (pred <= .3):\n",
        "      loc = 2\n",
        "    elif (pred <= .4):\n",
        "      loc = 3\n",
        "    elif (pred <= .5):\n",
        "      loc = 4\n",
        "    elif (pred <= .6):\n",
        "      loc = 5\n",
        "    elif (pred <= .7):\n",
        "      loc = 6\n",
        "    elif (pred <= .8):\n",
        "      loc = 7\n",
        "    elif (pred <= .9):\n",
        "      loc = 8\n",
        "    else:\n",
        "      loc = 9\n",
        "    if (gender[i] == 1): #one means man\n",
        "      sex_m_x[loc] = sex_m_x[loc] + 1\n",
        "    else:\n",
        "      sex_f_x[loc] = sex_f_x[loc] + 1\n",
        "\n",
        "  return sex_m_x, sex_f_x\n",
        "\n",
        "def kl_divergence(p, q):\n",
        "    #input: histogram distributions of probability counts, 10 bins from [0, 1]\n",
        "    #output: kullbach-leibler divergence D(p || q)\n",
        "\n",
        "    #first, we normalize the counts distribution to create a pmf (probability mass function)\n",
        "    p_pmf = p/np.sum(p)\n",
        "    q_pmf = q/np.sum(q)\n",
        "\n",
        "    #second, we compute the kullbach-leibler divergence\n",
        "    return np.sum(np.where(p_pmf != 0, p_pmf * np.log(p_pmf / q_pmf), 0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vYoegZNGpauS",
        "colab_type": "code",
        "outputId": "80adbad2-93cb-4e0a-cafb-27bfccc53023",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "def fit_curves(gan, model, x_train, y_train, gender_labels):\n",
        "  epsilon = 0.1\n",
        "  num_additions = 1000\n",
        "  divergence = 1 \n",
        "  \n",
        "  current_values = model.test(x_train)\n",
        "  mp, fp = bin_probabilities(current_values, gender_labels)\n",
        "  print(\"BEFORE\")\n",
        "  print(mp)\n",
        "  print(fp)\n",
        "\n",
        "  x_test = x_train\n",
        "  y_test = y_train\n",
        "  \n",
        "  num_iters = 204\n",
        "  divergence_vals = np.zeros(num_iters)\n",
        "  #while divergence > epsilon:\n",
        "  for i in range(num_iters):\n",
        "    # generate fake data\n",
        "    new_x, new_y = gan.generate_data(num_additions)\n",
        "    new_y = to_categorical(new_y)\n",
        "\n",
        "    # supplement x_train\n",
        "    x_train = np.append(x_train, new_x, axis=0)\n",
        "    y_train = np.append(y_train, new_y, axis=0)\n",
        "\n",
        "    # retrain model\n",
        "    model.train(x_train, y_train, x_test, y_test)\n",
        "    \n",
        "    # generate new curves\n",
        "    predictions = model.test(x_test)\n",
        "\n",
        "    # bin prediction values to create discrete count distribution\n",
        "    male_probabilities, female_probabilities = bin_probabilities(predictions, gender_labels)\n",
        "\n",
        "    # recalculate divergence\n",
        "    divergence = kl_divergence(male_probabilities, female_probabilities)\n",
        "    divergence_vals[i] = divergence\n",
        "    \n",
        "    # reset points\n",
        "    current_values = predictions\n",
        "  \n",
        "  print(\"AFTER\")\n",
        "  pred = model.test(x_test)\n",
        "  mpa, fpa = bin_probabilities(pred, gender_labels)\n",
        "  print(mpa)\n",
        "  print(fpa)\n",
        "  print(divergence_vals)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    x_train, y_train, gender_labels = process_data()\n",
        "    y_train = to_categorical(y_train)\n",
        "\n",
        "    gan.train(epochs=100, batch_size=100)\n",
        "    \n",
        "    model = PredictionModel()\n",
        "    model.train(x_train, y_train, x_train, y_train)\n",
        "\n",
        "    fit_curves(gan, model, x_train, y_train, gender_labels)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.360994, acc.: 82.50%] [G loss: 0.811080]\n",
            "1 [D loss: 0.383224, acc.: 79.50%] [G loss: 0.830939]\n",
            "2 [D loss: 0.325848, acc.: 84.50%] [G loss: 0.865797]\n",
            "3 [D loss: 0.337978, acc.: 86.00%] [G loss: 0.886582]\n",
            "4 [D loss: 0.350713, acc.: 84.50%] [G loss: 0.864081]\n",
            "5 [D loss: 0.352703, acc.: 84.50%] [G loss: 0.763999]\n",
            "6 [D loss: 0.352071, acc.: 83.00%] [G loss: 0.825192]\n",
            "7 [D loss: 0.385996, acc.: 77.50%] [G loss: 0.832678]\n",
            "8 [D loss: 0.330876, acc.: 87.00%] [G loss: 0.818205]\n",
            "9 [D loss: 0.335974, acc.: 88.00%] [G loss: 0.886918]\n",
            "10 [D loss: 0.351367, acc.: 84.50%] [G loss: 0.930830]\n",
            "11 [D loss: 0.327271, acc.: 83.50%] [G loss: 0.878874]\n",
            "12 [D loss: 0.304768, acc.: 90.50%] [G loss: 0.871836]\n",
            "13 [D loss: 0.351573, acc.: 82.00%] [G loss: 0.867025]\n",
            "14 [D loss: 0.417259, acc.: 71.00%] [G loss: 0.844203]\n",
            "15 [D loss: 0.339024, acc.: 84.50%] [G loss: 0.887761]\n",
            "16 [D loss: 0.344979, acc.: 82.50%] [G loss: 0.885757]\n",
            "17 [D loss: 0.322964, acc.: 86.00%] [G loss: 0.919067]\n",
            "18 [D loss: 0.322219, acc.: 87.50%] [G loss: 0.872057]\n",
            "19 [D loss: 0.350691, acc.: 81.50%] [G loss: 0.876864]\n",
            "20 [D loss: 0.343533, acc.: 81.50%] [G loss: 0.871120]\n",
            "21 [D loss: 0.349589, acc.: 86.00%] [G loss: 0.832208]\n",
            "22 [D loss: 0.329940, acc.: 85.00%] [G loss: 0.842341]\n",
            "23 [D loss: 0.368822, acc.: 77.00%] [G loss: 0.815300]\n",
            "24 [D loss: 0.356259, acc.: 80.50%] [G loss: 0.845142]\n",
            "25 [D loss: 0.323652, acc.: 87.00%] [G loss: 0.899959]\n",
            "26 [D loss: 0.317063, acc.: 86.50%] [G loss: 0.753675]\n",
            "27 [D loss: 0.314299, acc.: 86.50%] [G loss: 0.807301]\n",
            "28 [D loss: 0.352194, acc.: 81.50%] [G loss: 0.739890]\n",
            "29 [D loss: 0.404405, acc.: 71.00%] [G loss: 0.731502]\n",
            "30 [D loss: 0.382398, acc.: 77.50%] [G loss: 0.809302]\n",
            "31 [D loss: 0.306880, acc.: 89.50%] [G loss: 0.819533]\n",
            "32 [D loss: 0.316001, acc.: 84.00%] [G loss: 0.862679]\n",
            "33 [D loss: 0.306236, acc.: 85.00%] [G loss: 0.836810]\n",
            "34 [D loss: 0.363672, acc.: 79.50%] [G loss: 0.793507]\n",
            "35 [D loss: 0.361081, acc.: 80.50%] [G loss: 0.796557]\n",
            "36 [D loss: 0.343948, acc.: 84.50%] [G loss: 0.789809]\n",
            "37 [D loss: 0.337412, acc.: 84.00%] [G loss: 0.767379]\n",
            "38 [D loss: 0.327164, acc.: 81.50%] [G loss: 0.828371]\n",
            "39 [D loss: 0.348393, acc.: 83.00%] [G loss: 0.906686]\n",
            "40 [D loss: 0.309499, acc.: 82.50%] [G loss: 0.740809]\n",
            "41 [D loss: 0.356767, acc.: 78.50%] [G loss: 0.821913]\n",
            "42 [D loss: 0.343646, acc.: 81.50%] [G loss: 0.701568]\n",
            "43 [D loss: 0.373856, acc.: 80.50%] [G loss: 0.732418]\n",
            "44 [D loss: 0.241530, acc.: 92.00%] [G loss: 0.766527]\n",
            "45 [D loss: 0.340710, acc.: 86.00%] [G loss: 0.579978]\n",
            "46 [D loss: 0.377931, acc.: 84.50%] [G loss: 0.586669]\n",
            "47 [D loss: 0.274459, acc.: 87.00%] [G loss: 0.552904]\n",
            "48 [D loss: 0.266131, acc.: 88.00%] [G loss: 0.650160]\n",
            "49 [D loss: 0.319955, acc.: 86.50%] [G loss: 0.527533]\n",
            "50 [D loss: 0.245868, acc.: 90.00%] [G loss: 0.700769]\n",
            "51 [D loss: 0.245653, acc.: 89.50%] [G loss: 0.410557]\n",
            "52 [D loss: 0.217793, acc.: 91.50%] [G loss: 0.766376]\n",
            "53 [D loss: 0.309752, acc.: 87.00%] [G loss: 0.476912]\n",
            "54 [D loss: 0.350134, acc.: 87.00%] [G loss: 0.668532]\n",
            "55 [D loss: 0.494660, acc.: 76.50%] [G loss: 0.778026]\n",
            "56 [D loss: 0.103484, acc.: 98.00%] [G loss: 1.065306]\n",
            "57 [D loss: 0.366534, acc.: 86.50%] [G loss: 0.656090]\n",
            "58 [D loss: 0.332758, acc.: 92.00%] [G loss: 0.584122]\n",
            "59 [D loss: 0.376008, acc.: 83.50%] [G loss: 0.674628]\n",
            "60 [D loss: 0.236333, acc.: 92.50%] [G loss: 0.607812]\n",
            "61 [D loss: 0.419361, acc.: 90.50%] [G loss: 0.808499]\n",
            "62 [D loss: 0.381422, acc.: 88.50%] [G loss: 0.579573]\n",
            "63 [D loss: 0.411928, acc.: 89.00%] [G loss: 0.654829]\n",
            "64 [D loss: 0.414754, acc.: 88.50%] [G loss: 0.689377]\n",
            "65 [D loss: 0.267123, acc.: 91.00%] [G loss: 0.867366]\n",
            "66 [D loss: 0.518313, acc.: 82.00%] [G loss: 0.739011]\n",
            "67 [D loss: 0.509363, acc.: 85.00%] [G loss: 0.955970]\n",
            "68 [D loss: 0.203607, acc.: 94.00%] [G loss: 1.003234]\n",
            "69 [D loss: 0.518839, acc.: 88.00%] [G loss: 0.657728]\n",
            "70 [D loss: 0.744244, acc.: 80.50%] [G loss: 0.898543]\n",
            "71 [D loss: 0.258903, acc.: 91.00%] [G loss: 1.077201]\n",
            "72 [D loss: 0.373028, acc.: 84.50%] [G loss: 1.228252]\n",
            "73 [D loss: 0.336302, acc.: 86.50%] [G loss: 1.304488]\n",
            "74 [D loss: 0.392195, acc.: 90.00%] [G loss: 0.880542]\n",
            "75 [D loss: 0.425500, acc.: 86.00%] [G loss: 0.728864]\n",
            "76 [D loss: 0.405316, acc.: 89.50%] [G loss: 0.896020]\n",
            "77 [D loss: 0.329148, acc.: 86.50%] [G loss: 0.951706]\n",
            "78 [D loss: 0.322135, acc.: 88.50%] [G loss: 0.922569]\n",
            "79 [D loss: 0.265088, acc.: 91.00%] [G loss: 0.596938]\n",
            "80 [D loss: 0.284647, acc.: 91.50%] [G loss: 0.533357]\n",
            "81 [D loss: 0.344474, acc.: 88.50%] [G loss: 0.380268]\n",
            "82 [D loss: 0.234261, acc.: 92.00%] [G loss: 0.810313]\n",
            "83 [D loss: 0.285151, acc.: 87.00%] [G loss: 0.839004]\n",
            "84 [D loss: 0.296409, acc.: 90.00%] [G loss: 0.664752]\n",
            "85 [D loss: 0.363176, acc.: 86.50%] [G loss: 0.604569]\n",
            "86 [D loss: 0.315748, acc.: 92.50%] [G loss: 0.765640]\n",
            "87 [D loss: 0.162338, acc.: 94.50%] [G loss: 0.544816]\n",
            "88 [D loss: 0.412718, acc.: 88.50%] [G loss: 0.530584]\n",
            "89 [D loss: 0.100604, acc.: 97.00%] [G loss: 0.401414]\n",
            "90 [D loss: 0.075098, acc.: 98.50%] [G loss: 0.732468]\n",
            "91 [D loss: 0.100005, acc.: 97.50%] [G loss: 0.330145]\n",
            "92 [D loss: 0.249428, acc.: 89.50%] [G loss: 0.416016]\n",
            "93 [D loss: 0.137179, acc.: 95.00%] [G loss: 0.361172]\n",
            "94 [D loss: 0.083026, acc.: 98.00%] [G loss: 0.356569]\n",
            "95 [D loss: 0.075919, acc.: 99.00%] [G loss: 0.322055]\n",
            "96 [D loss: 0.108510, acc.: 97.00%] [G loss: 0.221856]\n",
            "97 [D loss: 0.239867, acc.: 93.00%] [G loss: 0.442007]\n",
            "98 [D loss: 0.075024, acc.: 99.00%] [G loss: 0.397833]\n",
            "99 [D loss: 0.126594, acc.: 96.50%] [G loss: 0.430876]\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_13 (Dense)             (None, 32)                3488      \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 2)                 66        \n",
            "=================================================================\n",
            "Total params: 5,666\n",
            "Trainable params: 5,666\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 32561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "32561/32561 [==============================] - 5s 158us/step - loss: 0.3921 - acc: 0.8179 - val_loss: 0.3379 - val_acc: 0.8414\n",
            "BEFORE\n",
            "[6857. 2790. 2592. 1869. 1155. 1151. 1629. 2544. 1126.   77.]\n",
            "[7822.  990.  522.  321.  231.  230.  318.  279.   58.    0.]\n",
            "Train on 33561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "33561/33561 [==============================] - 4s 133us/step - loss: 0.3440 - acc: 0.8423 - val_loss: 0.3310 - val_acc: 0.8443\n",
            "Train on 34561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "34561/34561 [==============================] - 4s 130us/step - loss: 0.3244 - acc: 0.8487 - val_loss: 0.3281 - val_acc: 0.8452\n",
            "Train on 35561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "35561/35561 [==============================] - 4s 125us/step - loss: 0.3114 - acc: 0.8559 - val_loss: 0.3250 - val_acc: 0.8480\n",
            "Train on 36561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "36561/36561 [==============================] - 5s 127us/step - loss: 0.3003 - acc: 0.8624 - val_loss: 0.3246 - val_acc: 0.8497\n",
            "Train on 37561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "37561/37561 [==============================] - 5s 128us/step - loss: 0.2896 - acc: 0.8674 - val_loss: 0.3199 - val_acc: 0.8505\n",
            "Train on 38561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "38561/38561 [==============================] - 5s 126us/step - loss: 0.2781 - acc: 0.8715 - val_loss: 0.3180 - val_acc: 0.8527\n",
            "Train on 39561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "39561/39561 [==============================] - 5s 124us/step - loss: 0.2716 - acc: 0.8759 - val_loss: 0.3163 - val_acc: 0.8548\n",
            "Train on 40561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "40561/40561 [==============================] - 5s 122us/step - loss: 0.2619 - acc: 0.8806 - val_loss: 0.3142 - val_acc: 0.8559\n",
            "Train on 41561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "41561/41561 [==============================] - 5s 126us/step - loss: 0.2555 - acc: 0.8823 - val_loss: 0.3119 - val_acc: 0.8564\n",
            "Train on 42561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "42561/42561 [==============================] - 5s 121us/step - loss: 0.2491 - acc: 0.8868 - val_loss: 0.3116 - val_acc: 0.8566\n",
            "Train on 43561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "43561/43561 [==============================] - 5s 123us/step - loss: 0.2428 - acc: 0.8891 - val_loss: 0.3104 - val_acc: 0.8578\n",
            "Train on 44561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "44561/44561 [==============================] - 5s 122us/step - loss: 0.2353 - acc: 0.8916 - val_loss: 0.3073 - val_acc: 0.8590\n",
            "Train on 45561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "45561/45561 [==============================] - 6s 121us/step - loss: 0.2287 - acc: 0.8942 - val_loss: 0.3082 - val_acc: 0.8576\n",
            "Train on 46561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "46561/46561 [==============================] - 6s 122us/step - loss: 0.2239 - acc: 0.8973 - val_loss: 0.3059 - val_acc: 0.8595\n",
            "Train on 47561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "47561/47561 [==============================] - 6s 120us/step - loss: 0.2182 - acc: 0.8994 - val_loss: 0.3038 - val_acc: 0.8601\n",
            "Train on 48561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "48561/48561 [==============================] - 6s 118us/step - loss: 0.2135 - acc: 0.9020 - val_loss: 0.3027 - val_acc: 0.8610\n",
            "Train on 49561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "49561/49561 [==============================] - 6s 118us/step - loss: 0.2087 - acc: 0.9038 - val_loss: 0.3008 - val_acc: 0.8618\n",
            "Train on 50561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "50561/50561 [==============================] - 6s 124us/step - loss: 0.2033 - acc: 0.9080 - val_loss: 0.3048 - val_acc: 0.8631\n",
            "Train on 51561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "51561/51561 [==============================] - 6s 118us/step - loss: 0.2010 - acc: 0.9085 - val_loss: 0.3052 - val_acc: 0.8596\n",
            "Train on 52561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "52561/52561 [==============================] - 6s 117us/step - loss: 0.1951 - acc: 0.9119 - val_loss: 0.3004 - val_acc: 0.8623\n",
            "Train on 53561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "53561/53561 [==============================] - 6s 117us/step - loss: 0.1909 - acc: 0.9124 - val_loss: 0.3031 - val_acc: 0.8608\n",
            "Train on 54561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "54561/54561 [==============================] - 6s 117us/step - loss: 0.1875 - acc: 0.9139 - val_loss: 0.2970 - val_acc: 0.8645\n",
            "Train on 55561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "55561/55561 [==============================] - 7s 117us/step - loss: 0.1834 - acc: 0.9163 - val_loss: 0.2961 - val_acc: 0.8645\n",
            "Train on 56561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "56561/56561 [==============================] - 7s 117us/step - loss: 0.1798 - acc: 0.9174 - val_loss: 0.2975 - val_acc: 0.8645\n",
            "Train on 57561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "57561/57561 [==============================] - 7s 116us/step - loss: 0.1769 - acc: 0.9186 - val_loss: 0.2984 - val_acc: 0.8642\n",
            "Train on 58561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "58561/58561 [==============================] - 7s 117us/step - loss: 0.1735 - acc: 0.9207 - val_loss: 0.2984 - val_acc: 0.8649\n",
            "Train on 59561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "59561/59561 [==============================] - 7s 118us/step - loss: 0.1703 - acc: 0.9218 - val_loss: 0.2963 - val_acc: 0.8635\n",
            "Train on 60561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "60561/60561 [==============================] - 7s 115us/step - loss: 0.1676 - acc: 0.9235 - val_loss: 0.2956 - val_acc: 0.8651\n",
            "Train on 61561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "61561/61561 [==============================] - 7s 116us/step - loss: 0.1655 - acc: 0.9240 - val_loss: 0.2972 - val_acc: 0.8635\n",
            "Train on 62561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "62561/62561 [==============================] - 7s 114us/step - loss: 0.1620 - acc: 0.9264 - val_loss: 0.2967 - val_acc: 0.8632\n",
            "Train on 63561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "63561/63561 [==============================] - 7s 116us/step - loss: 0.1593 - acc: 0.9271 - val_loss: 0.2936 - val_acc: 0.8663\n",
            "Train on 64561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "64561/64561 [==============================] - 8s 119us/step - loss: 0.1561 - acc: 0.9285 - val_loss: 0.2928 - val_acc: 0.8652\n",
            "Train on 65561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "65561/65561 [==============================] - 8s 115us/step - loss: 0.1539 - acc: 0.9294 - val_loss: 0.2956 - val_acc: 0.8644\n",
            "Train on 66561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "66561/66561 [==============================] - 8s 115us/step - loss: 0.1507 - acc: 0.9313 - val_loss: 0.2918 - val_acc: 0.8663\n",
            "Train on 67561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "67561/67561 [==============================] - 8s 112us/step - loss: 0.1491 - acc: 0.9317 - val_loss: 0.2937 - val_acc: 0.8669\n",
            "Train on 68561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "68561/68561 [==============================] - 8s 114us/step - loss: 0.1460 - acc: 0.9332 - val_loss: 0.2917 - val_acc: 0.8659\n",
            "Train on 69561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "69561/69561 [==============================] - 8s 113us/step - loss: 0.1445 - acc: 0.9342 - val_loss: 0.2913 - val_acc: 0.8688\n",
            "Train on 70561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "70561/70561 [==============================] - 8s 112us/step - loss: 0.1416 - acc: 0.9349 - val_loss: 0.2900 - val_acc: 0.8682\n",
            "Train on 71561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "71561/71561 [==============================] - 8s 115us/step - loss: 0.1395 - acc: 0.9371 - val_loss: 0.2900 - val_acc: 0.8689\n",
            "Train on 72561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "72561/72561 [==============================] - 8s 115us/step - loss: 0.1376 - acc: 0.9375 - val_loss: 0.2920 - val_acc: 0.8684\n",
            "Train on 73561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "73561/73561 [==============================] - 8s 115us/step - loss: 0.1344 - acc: 0.9390 - val_loss: 0.2884 - val_acc: 0.8697\n",
            "Train on 74561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "74561/74561 [==============================] - 8s 113us/step - loss: 0.1336 - acc: 0.9386 - val_loss: 0.2891 - val_acc: 0.8685\n",
            "Train on 75561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "75561/75561 [==============================] - 8s 112us/step - loss: 0.1317 - acc: 0.9400 - val_loss: 0.2874 - val_acc: 0.8685\n",
            "Train on 76561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "76561/76561 [==============================] - 8s 111us/step - loss: 0.1291 - acc: 0.9414 - val_loss: 0.2886 - val_acc: 0.8679\n",
            "Train on 77561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "77561/77561 [==============================] - 9s 110us/step - loss: 0.1284 - acc: 0.9405 - val_loss: 0.2855 - val_acc: 0.8705\n",
            "Train on 78561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "78561/78561 [==============================] - 9s 112us/step - loss: 0.1257 - acc: 0.9423 - val_loss: 0.2910 - val_acc: 0.8688\n",
            "Train on 79561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "79561/79561 [==============================] - 9s 111us/step - loss: 0.1239 - acc: 0.9438 - val_loss: 0.2869 - val_acc: 0.8690\n",
            "Train on 80561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "80561/80561 [==============================] - 9s 112us/step - loss: 0.1223 - acc: 0.9441 - val_loss: 0.2852 - val_acc: 0.8700\n",
            "Train on 81561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "81561/81561 [==============================] - 9s 110us/step - loss: 0.1208 - acc: 0.9449 - val_loss: 0.2874 - val_acc: 0.8698\n",
            "Train on 82561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "82561/82561 [==============================] - 9s 113us/step - loss: 0.1193 - acc: 0.9455 - val_loss: 0.2862 - val_acc: 0.8699\n",
            "Train on 83561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "83561/83561 [==============================] - 10s 115us/step - loss: 0.1180 - acc: 0.9458 - val_loss: 0.2849 - val_acc: 0.8702\n",
            "Train on 84561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "84561/84561 [==============================] - 10s 114us/step - loss: 0.1160 - acc: 0.9464 - val_loss: 0.2862 - val_acc: 0.8707\n",
            "Train on 85561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "85561/85561 [==============================] - 10s 111us/step - loss: 0.1156 - acc: 0.9468 - val_loss: 0.2858 - val_acc: 0.8705\n",
            "Train on 86561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "86561/86561 [==============================] - 10s 112us/step - loss: 0.1141 - acc: 0.9477 - val_loss: 0.2845 - val_acc: 0.8707\n",
            "Train on 87561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "87561/87561 [==============================] - 10s 113us/step - loss: 0.1118 - acc: 0.9486 - val_loss: 0.2839 - val_acc: 0.8715\n",
            "Train on 88561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "88561/88561 [==============================] - 10s 111us/step - loss: 0.1116 - acc: 0.9493 - val_loss: 0.2844 - val_acc: 0.8707\n",
            "Train on 89561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "89561/89561 [==============================] - 10s 110us/step - loss: 0.1095 - acc: 0.9499 - val_loss: 0.2843 - val_acc: 0.8699\n",
            "Train on 90561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "90561/90561 [==============================] - 10s 109us/step - loss: 0.1088 - acc: 0.9496 - val_loss: 0.2833 - val_acc: 0.8702\n",
            "Train on 91561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "91561/91561 [==============================] - 10s 111us/step - loss: 0.1070 - acc: 0.9509 - val_loss: 0.2845 - val_acc: 0.8714\n",
            "Train on 92561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "92561/92561 [==============================] - 10s 111us/step - loss: 0.1054 - acc: 0.9513 - val_loss: 0.2834 - val_acc: 0.8719\n",
            "Train on 93561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "93561/93561 [==============================] - 10s 111us/step - loss: 0.1047 - acc: 0.9524 - val_loss: 0.2830 - val_acc: 0.8702\n",
            "Train on 94561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "94561/94561 [==============================] - 10s 110us/step - loss: 0.1044 - acc: 0.9522 - val_loss: 0.2828 - val_acc: 0.8719\n",
            "Train on 95561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "95561/95561 [==============================] - 10s 109us/step - loss: 0.1021 - acc: 0.9533 - val_loss: 0.2839 - val_acc: 0.8691\n",
            "Train on 96561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "96561/96561 [==============================] - 11s 109us/step - loss: 0.1015 - acc: 0.9535 - val_loss: 0.2851 - val_acc: 0.8678\n",
            "Train on 97561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "97561/97561 [==============================] - 11s 111us/step - loss: 0.0998 - acc: 0.9543 - val_loss: 0.2859 - val_acc: 0.8712\n",
            "Train on 98561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "98561/98561 [==============================] - 11s 110us/step - loss: 0.1000 - acc: 0.9540 - val_loss: 0.2813 - val_acc: 0.8703\n",
            "Train on 99561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "99561/99561 [==============================] - 11s 109us/step - loss: 0.0979 - acc: 0.9552 - val_loss: 0.2860 - val_acc: 0.8676\n",
            "Train on 100561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "100561/100561 [==============================] - 11s 109us/step - loss: 0.0973 - acc: 0.9555 - val_loss: 0.2812 - val_acc: 0.8708\n",
            "Train on 101561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "101561/101561 [==============================] - 11s 109us/step - loss: 0.0959 - acc: 0.9556 - val_loss: 0.2822 - val_acc: 0.8714\n",
            "Train on 102561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "102561/102561 [==============================] - 11s 109us/step - loss: 0.0953 - acc: 0.9564 - val_loss: 0.2814 - val_acc: 0.8709\n",
            "Train on 103561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "103561/103561 [==============================] - 11s 109us/step - loss: 0.0947 - acc: 0.9567 - val_loss: 0.2830 - val_acc: 0.8706\n",
            "Train on 104561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "104561/104561 [==============================] - 11s 108us/step - loss: 0.0934 - acc: 0.9567 - val_loss: 0.2818 - val_acc: 0.8707\n",
            "Train on 105561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "105561/105561 [==============================] - 11s 108us/step - loss: 0.0924 - acc: 0.9577 - val_loss: 0.2852 - val_acc: 0.8697\n",
            "Train on 106561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "106561/106561 [==============================] - 11s 107us/step - loss: 0.0912 - acc: 0.9583 - val_loss: 0.2794 - val_acc: 0.8732\n",
            "Train on 107561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "107561/107561 [==============================] - 12s 108us/step - loss: 0.0902 - acc: 0.9583 - val_loss: 0.2803 - val_acc: 0.8716\n",
            "Train on 108561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "108561/108561 [==============================] - 12s 112us/step - loss: 0.0894 - acc: 0.9590 - val_loss: 0.2806 - val_acc: 0.8718\n",
            "Train on 109561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "109561/109561 [==============================] - 12s 106us/step - loss: 0.0892 - acc: 0.9592 - val_loss: 0.2806 - val_acc: 0.8712\n",
            "Train on 110561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "110561/110561 [==============================] - 12s 108us/step - loss: 0.0883 - acc: 0.9595 - val_loss: 0.2809 - val_acc: 0.8704\n",
            "Train on 111561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "111561/111561 [==============================] - 12s 107us/step - loss: 0.0874 - acc: 0.9601 - val_loss: 0.2789 - val_acc: 0.8720\n",
            "Train on 112561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "112561/112561 [==============================] - 12s 107us/step - loss: 0.0863 - acc: 0.9604 - val_loss: 0.2807 - val_acc: 0.8712\n",
            "Train on 113561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "113561/113561 [==============================] - 12s 109us/step - loss: 0.0862 - acc: 0.9604 - val_loss: 0.2828 - val_acc: 0.8707\n",
            "Train on 114561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "114561/114561 [==============================] - 12s 108us/step - loss: 0.0845 - acc: 0.9615 - val_loss: 0.2816 - val_acc: 0.8720\n",
            "Train on 115561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "115561/115561 [==============================] - 12s 107us/step - loss: 0.0844 - acc: 0.9612 - val_loss: 0.2798 - val_acc: 0.8711\n",
            "Train on 116561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "116561/116561 [==============================] - 13s 110us/step - loss: 0.0839 - acc: 0.9619 - val_loss: 0.2813 - val_acc: 0.8733\n",
            "Train on 117561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "117561/117561 [==============================] - 13s 109us/step - loss: 0.0825 - acc: 0.9624 - val_loss: 0.2791 - val_acc: 0.8709\n",
            "Train on 118561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "118561/118561 [==============================] - 13s 107us/step - loss: 0.0819 - acc: 0.9629 - val_loss: 0.2796 - val_acc: 0.8723\n",
            "Train on 119561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "119561/119561 [==============================] - 13s 107us/step - loss: 0.0815 - acc: 0.9629 - val_loss: 0.2777 - val_acc: 0.8731\n",
            "Train on 120561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "120561/120561 [==============================] - 13s 109us/step - loss: 0.0804 - acc: 0.9631 - val_loss: 0.2784 - val_acc: 0.8733\n",
            "Train on 121561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "121561/121561 [==============================] - 13s 108us/step - loss: 0.0798 - acc: 0.9637 - val_loss: 0.2788 - val_acc: 0.8723\n",
            "Train on 122561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "122561/122561 [==============================] - 13s 108us/step - loss: 0.0795 - acc: 0.9639 - val_loss: 0.2813 - val_acc: 0.8729\n",
            "Train on 123561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "123561/123561 [==============================] - 13s 106us/step - loss: 0.0787 - acc: 0.9641 - val_loss: 0.2788 - val_acc: 0.8720\n",
            "Train on 124561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "124561/124561 [==============================] - 13s 106us/step - loss: 0.0779 - acc: 0.9643 - val_loss: 0.2806 - val_acc: 0.8707\n",
            "Train on 125561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "125561/125561 [==============================] - 13s 106us/step - loss: 0.0780 - acc: 0.9645 - val_loss: 0.2785 - val_acc: 0.8710\n",
            "Train on 126561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "126561/126561 [==============================] - 13s 106us/step - loss: 0.0770 - acc: 0.9648 - val_loss: 0.2801 - val_acc: 0.8724\n",
            "Train on 127561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "127561/127561 [==============================] - 13s 106us/step - loss: 0.0763 - acc: 0.9651 - val_loss: 0.2821 - val_acc: 0.8714\n",
            "Train on 128561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "128561/128561 [==============================] - 14s 106us/step - loss: 0.0754 - acc: 0.9656 - val_loss: 0.2788 - val_acc: 0.8728\n",
            "Train on 129561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "129561/129561 [==============================] - 14s 111us/step - loss: 0.0752 - acc: 0.9658 - val_loss: 0.2773 - val_acc: 0.8721\n",
            "Train on 130561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "130561/130561 [==============================] - 14s 107us/step - loss: 0.0740 - acc: 0.9660 - val_loss: 0.2772 - val_acc: 0.8733\n",
            "Train on 131561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "131561/131561 [==============================] - 14s 107us/step - loss: 0.0735 - acc: 0.9663 - val_loss: 0.2756 - val_acc: 0.8749\n",
            "Train on 132561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "132561/132561 [==============================] - 14s 106us/step - loss: 0.0736 - acc: 0.9665 - val_loss: 0.2779 - val_acc: 0.8732\n",
            "Train on 133561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "133561/133561 [==============================] - 14s 107us/step - loss: 0.0724 - acc: 0.9665 - val_loss: 0.2779 - val_acc: 0.8753\n",
            "Train on 134561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "134561/134561 [==============================] - 14s 105us/step - loss: 0.0719 - acc: 0.9672 - val_loss: 0.2774 - val_acc: 0.8732\n",
            "Train on 135561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "135561/135561 [==============================] - 14s 107us/step - loss: 0.0711 - acc: 0.9673 - val_loss: 0.2780 - val_acc: 0.8727\n",
            "Train on 136561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "136561/136561 [==============================] - 15s 108us/step - loss: 0.0714 - acc: 0.9675 - val_loss: 0.2759 - val_acc: 0.8744\n",
            "Train on 137561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "137561/137561 [==============================] - 15s 106us/step - loss: 0.0703 - acc: 0.9679 - val_loss: 0.2775 - val_acc: 0.8748\n",
            "Train on 138561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "138561/138561 [==============================] - 15s 105us/step - loss: 0.0698 - acc: 0.9681 - val_loss: 0.2786 - val_acc: 0.8727\n",
            "Train on 139561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "139561/139561 [==============================] - 15s 105us/step - loss: 0.0698 - acc: 0.9678 - val_loss: 0.2813 - val_acc: 0.8732\n",
            "Train on 140561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "140561/140561 [==============================] - 15s 105us/step - loss: 0.0686 - acc: 0.9684 - val_loss: 0.2769 - val_acc: 0.8730\n",
            "Train on 141561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "141561/141561 [==============================] - 15s 104us/step - loss: 0.0679 - acc: 0.9693 - val_loss: 0.2771 - val_acc: 0.8742\n",
            "Train on 142561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "142561/142561 [==============================] - 15s 105us/step - loss: 0.0678 - acc: 0.9689 - val_loss: 0.2774 - val_acc: 0.8741\n",
            "Train on 143561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "143561/143561 [==============================] - 15s 105us/step - loss: 0.0672 - acc: 0.9689 - val_loss: 0.2788 - val_acc: 0.8725\n",
            "Train on 144561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "144561/144561 [==============================] - 15s 106us/step - loss: 0.0671 - acc: 0.9693 - val_loss: 0.2755 - val_acc: 0.8742\n",
            "Train on 145561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "145561/145561 [==============================] - 16s 107us/step - loss: 0.0666 - acc: 0.9698 - val_loss: 0.2785 - val_acc: 0.8731\n",
            "Train on 146561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "146561/146561 [==============================] - 15s 106us/step - loss: 0.0659 - acc: 0.9697 - val_loss: 0.2785 - val_acc: 0.8723\n",
            "Train on 147561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "147561/147561 [==============================] - 15s 105us/step - loss: 0.0656 - acc: 0.9699 - val_loss: 0.2770 - val_acc: 0.8737\n",
            "Train on 148561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "148561/148561 [==============================] - 16s 108us/step - loss: 0.0650 - acc: 0.9705 - val_loss: 0.2746 - val_acc: 0.8747\n",
            "Train on 149561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "149561/149561 [==============================] - 16s 106us/step - loss: 0.0646 - acc: 0.9704 - val_loss: 0.2770 - val_acc: 0.8740\n",
            "Train on 150561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "150561/150561 [==============================] - 16s 105us/step - loss: 0.0639 - acc: 0.9707 - val_loss: 0.2761 - val_acc: 0.8740\n",
            "Train on 151561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "151561/151561 [==============================] - 16s 105us/step - loss: 0.0636 - acc: 0.9705 - val_loss: 0.2778 - val_acc: 0.8760\n",
            "Train on 152561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "152561/152561 [==============================] - 16s 105us/step - loss: 0.0634 - acc: 0.9714 - val_loss: 0.2773 - val_acc: 0.8743\n",
            "Train on 153561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "153561/153561 [==============================] - 16s 105us/step - loss: 0.0626 - acc: 0.9713 - val_loss: 0.2773 - val_acc: 0.8735\n",
            "Train on 154561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "154561/154561 [==============================] - 16s 106us/step - loss: 0.0625 - acc: 0.9713 - val_loss: 0.2751 - val_acc: 0.8746\n",
            "Train on 155561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "155561/155561 [==============================] - 16s 105us/step - loss: 0.0617 - acc: 0.9719 - val_loss: 0.2750 - val_acc: 0.8750\n",
            "Train on 156561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "156561/156561 [==============================] - 16s 105us/step - loss: 0.0613 - acc: 0.9719 - val_loss: 0.2780 - val_acc: 0.8750\n",
            "Train on 157561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "157561/157561 [==============================] - 17s 105us/step - loss: 0.0612 - acc: 0.9723 - val_loss: 0.2739 - val_acc: 0.8752\n",
            "Train on 158561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "158561/158561 [==============================] - 17s 105us/step - loss: 0.0611 - acc: 0.9724 - val_loss: 0.2805 - val_acc: 0.8747\n",
            "Train on 159561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "159561/159561 [==============================] - 17s 105us/step - loss: 0.0603 - acc: 0.9724 - val_loss: 0.2743 - val_acc: 0.8754\n",
            "Train on 160561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "160561/160561 [==============================] - 17s 104us/step - loss: 0.0599 - acc: 0.9727 - val_loss: 0.2762 - val_acc: 0.8742\n",
            "Train on 161561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "161561/161561 [==============================] - 17s 103us/step - loss: 0.0593 - acc: 0.9730 - val_loss: 0.2755 - val_acc: 0.8756\n",
            "Train on 162561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "162561/162561 [==============================] - 17s 104us/step - loss: 0.0591 - acc: 0.9730 - val_loss: 0.2783 - val_acc: 0.8768\n",
            "Train on 163561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "163561/163561 [==============================] - 17s 104us/step - loss: 0.0588 - acc: 0.9733 - val_loss: 0.2757 - val_acc: 0.8750\n",
            "Train on 164561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "164561/164561 [==============================] - 17s 105us/step - loss: 0.0585 - acc: 0.9734 - val_loss: 0.2760 - val_acc: 0.8745\n",
            "Train on 165561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "165561/165561 [==============================] - 18s 107us/step - loss: 0.0582 - acc: 0.9733 - val_loss: 0.2747 - val_acc: 0.8751\n",
            "Train on 166561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "166561/166561 [==============================] - 18s 106us/step - loss: 0.0574 - acc: 0.9738 - val_loss: 0.2781 - val_acc: 0.8741\n",
            "Train on 167561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "167561/167561 [==============================] - 18s 104us/step - loss: 0.0573 - acc: 0.9741 - val_loss: 0.2750 - val_acc: 0.8748\n",
            "Train on 168561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "168561/168561 [==============================] - 18s 105us/step - loss: 0.0571 - acc: 0.9736 - val_loss: 0.2735 - val_acc: 0.8752\n",
            "Train on 169561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "169561/169561 [==============================] - 18s 104us/step - loss: 0.0566 - acc: 0.9743 - val_loss: 0.2739 - val_acc: 0.8746\n",
            "Train on 170561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "170561/170561 [==============================] - 18s 107us/step - loss: 0.0565 - acc: 0.9743 - val_loss: 0.2756 - val_acc: 0.8746\n",
            "Train on 171561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "171561/171561 [==============================] - 18s 104us/step - loss: 0.0564 - acc: 0.9740 - val_loss: 0.2743 - val_acc: 0.8753\n",
            "Train on 172561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "172561/172561 [==============================] - 18s 105us/step - loss: 0.0554 - acc: 0.9746 - val_loss: 0.2758 - val_acc: 0.8738\n",
            "Train on 173561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "173561/173561 [==============================] - 18s 105us/step - loss: 0.0553 - acc: 0.9747 - val_loss: 0.2756 - val_acc: 0.8753\n",
            "Train on 174561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "174561/174561 [==============================] - 18s 104us/step - loss: 0.0553 - acc: 0.9746 - val_loss: 0.2817 - val_acc: 0.8740\n",
            "Train on 175561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "175561/175561 [==============================] - 18s 105us/step - loss: 0.0547 - acc: 0.9748 - val_loss: 0.2800 - val_acc: 0.8737\n",
            "Train on 176561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "176561/176561 [==============================] - 18s 104us/step - loss: 0.0544 - acc: 0.9750 - val_loss: 0.2727 - val_acc: 0.8760\n",
            "Train on 177561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "177561/177561 [==============================] - 19s 105us/step - loss: 0.0546 - acc: 0.9747 - val_loss: 0.2775 - val_acc: 0.8756\n",
            "Train on 178561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "178561/178561 [==============================] - 19s 105us/step - loss: 0.0537 - acc: 0.9753 - val_loss: 0.2756 - val_acc: 0.8748\n",
            "Train on 179561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "179561/179561 [==============================] - 19s 105us/step - loss: 0.0538 - acc: 0.9751 - val_loss: 0.2770 - val_acc: 0.8758\n",
            "Train on 180561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "180561/180561 [==============================] - 19s 108us/step - loss: 0.0529 - acc: 0.9758 - val_loss: 0.2792 - val_acc: 0.8739\n",
            "Train on 181561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "181561/181561 [==============================] - 19s 105us/step - loss: 0.0529 - acc: 0.9757 - val_loss: 0.2733 - val_acc: 0.8745\n",
            "Train on 182561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "182561/182561 [==============================] - 19s 104us/step - loss: 0.0527 - acc: 0.9759 - val_loss: 0.2769 - val_acc: 0.8755\n",
            "Train on 183561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "183561/183561 [==============================] - 19s 105us/step - loss: 0.0523 - acc: 0.9757 - val_loss: 0.2730 - val_acc: 0.8759\n",
            "Train on 184561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "184561/184561 [==============================] - 19s 105us/step - loss: 0.0519 - acc: 0.9764 - val_loss: 0.2732 - val_acc: 0.8759\n",
            "Train on 185561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "185561/185561 [==============================] - 19s 105us/step - loss: 0.0516 - acc: 0.9762 - val_loss: 0.2757 - val_acc: 0.8753\n",
            "Train on 186561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "186561/186561 [==============================] - 19s 102us/step - loss: 0.0514 - acc: 0.9767 - val_loss: 0.2731 - val_acc: 0.8763\n",
            "Train on 187561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "187561/187561 [==============================] - 19s 101us/step - loss: 0.0511 - acc: 0.9767 - val_loss: 0.2759 - val_acc: 0.8742\n",
            "Train on 188561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "188561/188561 [==============================] - 20s 104us/step - loss: 0.0508 - acc: 0.9769 - val_loss: 0.2751 - val_acc: 0.8763\n",
            "Train on 189561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "189561/189561 [==============================] - 20s 104us/step - loss: 0.0504 - acc: 0.9770 - val_loss: 0.2765 - val_acc: 0.8748\n",
            "Train on 190561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "190561/190561 [==============================] - 20s 105us/step - loss: 0.0501 - acc: 0.9773 - val_loss: 0.2704 - val_acc: 0.8779\n",
            "Train on 191561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "191561/191561 [==============================] - 20s 104us/step - loss: 0.0503 - acc: 0.9770 - val_loss: 0.2737 - val_acc: 0.8753\n",
            "Train on 192561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "192561/192561 [==============================] - 20s 104us/step - loss: 0.0501 - acc: 0.9770 - val_loss: 0.2744 - val_acc: 0.8755\n",
            "Train on 193561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "193561/193561 [==============================] - 20s 104us/step - loss: 0.0497 - acc: 0.9772 - val_loss: 0.2725 - val_acc: 0.8759\n",
            "Train on 194561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "194561/194561 [==============================] - 20s 103us/step - loss: 0.0493 - acc: 0.9772 - val_loss: 0.2767 - val_acc: 0.8735\n",
            "Train on 195561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "195561/195561 [==============================] - 21s 107us/step - loss: 0.0487 - acc: 0.9776 - val_loss: 0.2739 - val_acc: 0.8736\n",
            "Train on 196561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "196561/196561 [==============================] - 20s 104us/step - loss: 0.0487 - acc: 0.9778 - val_loss: 0.2746 - val_acc: 0.8746\n",
            "Train on 197561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "197561/197561 [==============================] - 21s 104us/step - loss: 0.0484 - acc: 0.9778 - val_loss: 0.2740 - val_acc: 0.8742\n",
            "Train on 198561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "198561/198561 [==============================] - 21s 104us/step - loss: 0.0483 - acc: 0.9777 - val_loss: 0.2755 - val_acc: 0.8750\n",
            "Train on 199561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "199561/199561 [==============================] - 21s 105us/step - loss: 0.0481 - acc: 0.9781 - val_loss: 0.2736 - val_acc: 0.8745\n",
            "Train on 200561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "200561/200561 [==============================] - 21s 104us/step - loss: 0.0475 - acc: 0.9784 - val_loss: 0.2732 - val_acc: 0.8759\n",
            "Train on 201561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "201561/201561 [==============================] - 21s 103us/step - loss: 0.0481 - acc: 0.9779 - val_loss: 0.2835 - val_acc: 0.8728\n",
            "Train on 202561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "202561/202561 [==============================] - 21s 104us/step - loss: 0.0475 - acc: 0.9783 - val_loss: 0.2732 - val_acc: 0.8747\n",
            "Train on 203561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "203561/203561 [==============================] - 21s 103us/step - loss: 0.0471 - acc: 0.9784 - val_loss: 0.2749 - val_acc: 0.8762\n",
            "Train on 204561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "204561/204561 [==============================] - 21s 102us/step - loss: 0.0468 - acc: 0.9786 - val_loss: 0.2769 - val_acc: 0.8775\n",
            "Train on 205561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "205561/205561 [==============================] - 21s 104us/step - loss: 0.0466 - acc: 0.9786 - val_loss: 0.2721 - val_acc: 0.8772\n",
            "Train on 206561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "206561/206561 [==============================] - 22s 104us/step - loss: 0.0465 - acc: 0.9788 - val_loss: 0.2746 - val_acc: 0.8751\n",
            "Train on 207561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "207561/207561 [==============================] - 21s 103us/step - loss: 0.0461 - acc: 0.9789 - val_loss: 0.2730 - val_acc: 0.8769\n",
            "Train on 208561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "208561/208561 [==============================] - 22s 105us/step - loss: 0.0457 - acc: 0.9791 - val_loss: 0.2737 - val_acc: 0.8743\n",
            "Train on 209561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "209561/209561 [==============================] - 21s 102us/step - loss: 0.0459 - acc: 0.9791 - val_loss: 0.2726 - val_acc: 0.8765\n",
            "Train on 210561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "210561/210561 [==============================] - 22s 104us/step - loss: 0.0455 - acc: 0.9792 - val_loss: 0.2760 - val_acc: 0.8754\n",
            "Train on 211561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "211561/211561 [==============================] - 22s 104us/step - loss: 0.0456 - acc: 0.9791 - val_loss: 0.2734 - val_acc: 0.8755\n",
            "Train on 212561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "212561/212561 [==============================] - 22s 106us/step - loss: 0.0449 - acc: 0.9795 - val_loss: 0.2727 - val_acc: 0.8783\n",
            "Train on 213561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "213561/213561 [==============================] - 22s 104us/step - loss: 0.0450 - acc: 0.9796 - val_loss: 0.2756 - val_acc: 0.8748\n",
            "Train on 214561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "214561/214561 [==============================] - 22s 104us/step - loss: 0.0447 - acc: 0.9797 - val_loss: 0.2741 - val_acc: 0.8759\n",
            "Train on 215561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "215561/215561 [==============================] - 22s 104us/step - loss: 0.0442 - acc: 0.9797 - val_loss: 0.2722 - val_acc: 0.8749\n",
            "Train on 216561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "216561/216561 [==============================] - 22s 102us/step - loss: 0.0439 - acc: 0.9797 - val_loss: 0.2736 - val_acc: 0.8757\n",
            "Train on 217561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "217561/217561 [==============================] - 23s 104us/step - loss: 0.0437 - acc: 0.9800 - val_loss: 0.2717 - val_acc: 0.8779\n",
            "Train on 218561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "218561/218561 [==============================] - 23s 103us/step - loss: 0.0440 - acc: 0.9800 - val_loss: 0.2718 - val_acc: 0.8760\n",
            "Train on 219561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "219561/219561 [==============================] - 23s 104us/step - loss: 0.0435 - acc: 0.9803 - val_loss: 0.2716 - val_acc: 0.8765\n",
            "Train on 220561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "220561/220561 [==============================] - 23s 104us/step - loss: 0.0431 - acc: 0.9802 - val_loss: 0.2711 - val_acc: 0.8768\n",
            "Train on 221561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "221561/221561 [==============================] - 24s 106us/step - loss: 0.0430 - acc: 0.9803 - val_loss: 0.2702 - val_acc: 0.8774\n",
            "Train on 222561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "222561/222561 [==============================] - 23s 104us/step - loss: 0.0429 - acc: 0.9803 - val_loss: 0.2733 - val_acc: 0.8760\n",
            "Train on 223561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "223561/223561 [==============================] - 23s 104us/step - loss: 0.0430 - acc: 0.9804 - val_loss: 0.2696 - val_acc: 0.8774\n",
            "Train on 224561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "224561/224561 [==============================] - 24s 105us/step - loss: 0.0427 - acc: 0.9805 - val_loss: 0.2731 - val_acc: 0.8756\n",
            "Train on 225561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "225561/225561 [==============================] - 23s 104us/step - loss: 0.0422 - acc: 0.9806 - val_loss: 0.2764 - val_acc: 0.8741\n",
            "Train on 226561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "226561/226561 [==============================] - 24s 104us/step - loss: 0.0420 - acc: 0.9809 - val_loss: 0.2722 - val_acc: 0.8744\n",
            "Train on 227561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "227561/227561 [==============================] - 24s 103us/step - loss: 0.0417 - acc: 0.9808 - val_loss: 0.2733 - val_acc: 0.8773\n",
            "Train on 228561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "228561/228561 [==============================] - 24s 103us/step - loss: 0.0417 - acc: 0.9810 - val_loss: 0.2732 - val_acc: 0.8756\n",
            "Train on 229561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "229561/229561 [==============================] - 24s 103us/step - loss: 0.0416 - acc: 0.9809 - val_loss: 0.2731 - val_acc: 0.8762\n",
            "Train on 230561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "230561/230561 [==============================] - 24s 104us/step - loss: 0.0416 - acc: 0.9810 - val_loss: 0.2745 - val_acc: 0.8756\n",
            "Train on 231561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "231561/231561 [==============================] - 24s 103us/step - loss: 0.0414 - acc: 0.9811 - val_loss: 0.2748 - val_acc: 0.8764\n",
            "Train on 232561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "232561/232561 [==============================] - 24s 104us/step - loss: 0.0408 - acc: 0.9814 - val_loss: 0.2744 - val_acc: 0.8745\n",
            "Train on 233561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "233561/233561 [==============================] - 25s 106us/step - loss: 0.0409 - acc: 0.9813 - val_loss: 0.2687 - val_acc: 0.8779\n",
            "Train on 234561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "234561/234561 [==============================] - 24s 103us/step - loss: 0.0406 - acc: 0.9815 - val_loss: 0.2704 - val_acc: 0.8767\n",
            "Train on 235561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "235561/235561 [==============================] - 24s 104us/step - loss: 0.0405 - acc: 0.9812 - val_loss: 0.2694 - val_acc: 0.8782\n",
            "Train on 236561 samples, validate on 32561 samples\n",
            "Epoch 1/1\n",
            "236561/236561 [==============================] - 25s 104us/step - loss: 0.0399 - acc: 0.9819 - val_loss: 0.2708 - val_acc: 0.8774\n",
            "AFTER\n",
            "[8515. 2672. 2115. 1445. 1341. 1296. 1320. 1300.  627. 1159.]\n",
            "[8143.  862.  415.  242.  192.  231.  235.  190.   77.  184.]\n",
            "[0.45859574 0.44499282 0.38055853 0.41209765 0.42078406 0.38109891\n",
            " 0.42864321 0.41198186 0.37426708 0.36853494 0.40282562 0.41478784\n",
            " 0.43177165 0.37624996 0.39081996 0.38994076 0.38741833 0.42866261\n",
            " 0.38407298 0.4080242  0.35950426 0.36776506 0.35063535 0.39630271\n",
            " 0.35019283 0.38081144 0.40808756 0.39096695 0.37870373 0.3685967\n",
            " 0.37552496 0.36291811 0.32797425 0.36709253 0.38159202 0.38934637\n",
            " 0.36828915 0.37597399 0.40474412 0.3743984  0.3721019  0.35354702\n",
            " 0.3819747  0.39607735 0.37609242 0.37400125 0.35879253 0.39415634\n",
            " 0.38627999 0.4194066  0.41466334 0.36574052 0.38705932 0.39353121\n",
            " 0.35127703 0.39991266 0.37183693 0.3636358  0.35474672 0.38662083\n",
            " 0.33746706 0.38105373 0.36918131 0.39436281 0.36367147 0.40688845\n",
            " 0.36071825 0.35747385 0.37208065 0.35363244 0.41697365 0.38110509\n",
            " 0.35624892 0.39128318 0.35842837 0.36060194 0.36545491 0.39731657\n",
            " 0.35267858 0.35447694 0.41149165 0.34362386 0.4020043  0.35284267\n",
            " 0.3622043  0.39232381 0.36313893 0.39269283 0.34897184 0.35227334\n",
            " 0.37276242 0.35557427 0.36099891 0.3572098  0.36876037 0.36009816\n",
            " 0.35362751 0.37121785 0.35234703 0.33579369 0.39560128 0.33625687\n",
            " 0.3311278  0.40964963 0.36713637 0.36226432 0.34943334 0.39585075\n",
            " 0.36942254 0.39179625 0.3946181  0.38094653 0.36934104 0.36991097\n",
            " 0.35250772 0.39111807 0.36106283 0.36472952 0.38719218 0.34709304\n",
            " 0.38683207 0.3802575  0.41345717 0.36171353 0.35872663 0.3889001\n",
            " 0.38381524 0.3790076  0.3521586  0.35544847 0.40754345 0.36082369\n",
            " 0.39323874 0.38903421 0.368057   0.36879494 0.34467932 0.36148526\n",
            " 0.34057746 0.39990737 0.3487209  0.37077806 0.34909751 0.32539913\n",
            " 0.37583295 0.35369382 0.39373273 0.3791521  0.37190425 0.35350912\n",
            " 0.35998784 0.33402842 0.37832147 0.38667107 0.36409671 0.37314035\n",
            " 0.35971784 0.34810447 0.38398916 0.38011086 0.37636266 0.37529036\n",
            " 0.38445216 0.33218035 0.37755155 0.36415718 0.37973315 0.36345435\n",
            " 0.37716685 0.38691075 0.36218672 0.40169464 0.37662076 0.334982\n",
            " 0.37870263 0.35136321 0.35056944 0.35011677 0.35634625 0.37900927\n",
            " 0.3477023  0.3931903  0.38001332 0.38552132 0.37189892 0.41111293\n",
            " 0.39049836 0.36287741 0.37915149 0.35670961 0.36695495 0.35421399\n",
            " 0.38875321 0.32203592 0.35421779 0.38123486 0.36806856 0.34518094\n",
            " 0.37740614 0.38547826 0.35049658 0.36098434 0.35499304 0.32786575]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qIyiV1RQUCm",
        "colab_type": "code",
        "outputId": "e4d26924-f036-4bcd-e301-6a1e3cb530df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 831
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "bins = np.zeros(10)\n",
        "for i in range(len(bins)):\n",
        "  bins[i] = 0.1*i + 0.1\n",
        "\n",
        "# PLOT BEFORE DATA GENERATION\n",
        "male_before = [6857., 2790., 2592., 1869., 1155., 1151., 1629., 2544., 1126.,   77.]\n",
        "female_before = [7822.,  990.,  522.,  321.,  231.,  230.,  318.,  279.,   58.,    0.]\n",
        "\n",
        "plt.xlabel(\"probability >50K income by bin\")\n",
        "plt.ylabel(\"number of individuals before generation\")\n",
        "\n",
        "plt.plot(bins, male_before, color=\"blue\")\n",
        "plt.plot(bins, female_before, color=\"red\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT AFTER DATA GENERATION\n",
        "plt.xlabel(\"probability >50K income by bin\")\n",
        "plt.ylabel(\"number of individuals after generation\")\n",
        "\n",
        "male_after = [8515., 2672., 2115., 1445., 1341., 1296., 1320., 1300.,  627., 1159.]\n",
        "female_after = [8143.,  862.,  415.,  242.,  192.,  231.,  235.,  190.,   77.,  184.]\n",
        "plt.plot(bins, male_after, color=\"blue\")\n",
        "plt.plot(bins, female_after, color=\"red\")\n",
        "plt.show()\n",
        "\n",
        "# PLOT WITH NORMALIZED VALUES AFTER DATA GENERATION\n",
        "plt.xlabel(\"probability >50K income by bin\")\n",
        "plt.ylabel(\"proportion of individuals after generation (normalized)\")\n",
        "\n",
        "male_after_norm = np.array(male_after) / np.array(male_after).sum()\n",
        "female_after_norm = np.array(female_after) / np.array(female_after).sum()\n",
        "\n",
        "plt.plot(bins, male_after_norm, color=\"blue\")\n",
        "plt.plot(bins, female_after_norm, color=\"red\")\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEGCAYAAACUzrmNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGXmycM45l9H/B2JLNcyU6644AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEiCAYAAAD05tVnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}